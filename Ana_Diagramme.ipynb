{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Entfernen aller \"-\" in Wörtern, welche nur als Absatz dienen und nicht andere grammatikalische Funktionen haben."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles=['Administrator','Apotheker','Arzt','Assistent','Techniker','Zeichner','Beamter','Berater','Betriebswirt','Ologe','Buchhalter','Chemiker','Controller','Designer','Direktor','Disponent','Elektriker','Entwickler','Fotograf','Friseur','Geomatiker','Gärtner','Informatiker','Ingenieur','Journalist','Jurist','Kaufmann','Koch','Koordinator','Laborant','Lehrer','Leiter','Maler','Manager','Maschinist','Mathematiker','Maurer','Mechatroniker','Mediengestalter','Meister','Musiker','fachmann','Physiker','Professor','Prüfer','anwalt','Redakteur','Referent','Richter','Schauspieler','Schneider','Schäfer','Sekretär','Bauer','Berufseinsteiger','Bewerber','Kollege','Architekt','Doktor','Erzieher','Verwaltungsfachwirt','Forscher','Pädagoge','Diplomverwaltungswirt','Fachagrarwirt','Planer','Werkstudent','Spezialist','Analyst','Psychotherapeut','Rettungsschwimmer','Systemintegrator','Amtsbote','Helfer','Elektroniker','doktorand','bibliothekar','bezirksschornsteinfeger','Elektromonteur','Fahrer','Pfleger','Diakon','Controllingexperte','Berufsjäger','Galvaniseur','Aufsichtshauer','Kämmerer','Recruiter','Tiergesundheitsaufseher','Schlosser','Programmierer','Angestellter','Praktiker','Sprecher','Binnenschiffer','Prozessberater','Mülllader','Maschinenbediener','Inspektor','Helfer','Seher','Tester','Führer','Agent','Zimmerer','Operator','Beschäftigter','Winzer','Taucher','Betreuer','Werker','Verwalter','Nautiker','Wärter','Wirtschafter','Übersetzer','Trainer','Dolmetscher','Developer','Mechaniker','Beauftragter','Sanitäter','Arbeiter','Ermittler','Wart','Schreiber','Überwacher','Wissenschaftler','Dachdecker','Ausbilder','Verantwortlicher','Beigeordneter','Reiniger','Beobachter','Vorsteher','Arbeitnehmer','dezernent','forstwirt','kontrolleur', 'Benutzer', 'Feuerwehrmann','Polizist']\n",
    "\n",
    "extra_femininum=['angestelltin','assistenz','beamtin','beamtinnen','ingenieurinnen','meisterin','meisterinnen','Ärztin','Ärztinnen','ologin','ologinnen','Köchin','Köchinnen','kauffrau','friseuse','fachfrau','anwältin','anwältinnen','bäuerin','amtsbotin','amtsbotinnen','Controllingexpertin','Controllingexpertinnen','kämmerin','kämmerinnen','zimmerin','zimmerinnen','Beauftragtin','Beauftragtinnen','beigeordneterin','beigeordneterinnen','pädagogin','pädagoginnen','bootsfrau', 'Feuerwehrfrau']\n",
    "\n",
    "extra_gm_plural=['angestellte','angestellten','beamte','ingenieure','Ärzte','ologen','Köche','anwälte','amtsboten','Controllingexperten','beschäftigte','Beauftragte','beigeordnete','verantwortliche','pädagogen','kolleg','Controllingexpert','Amtsbot','Pädagog','ingenieuren','arbeitenden','reinigungskraft','beamten','lehrkraft','schreibkraft','fachkraft','hilfskraft','assistenzkraft']\n",
    "\n",
    "berufe=['Administrator','Apotheker','Assistent','Techniker','Zeichner','Berater','Betriebswirt','Buchhalter','Chemiker','Controller','friseur','Designer','Direktor','Disponent','Elektriker','Entwickler','Fotograf','Friseur','Geomatiker','Gärtner','Informatiker','Journalist','Jurist','Koch','Koordinator','Laborant','Lehrer','Leiter','Maler','Manager','Maschinist','Mathematiker','Maurer','Mechatroniker','Mediengestalter','Meister','Musiker','Physiker','Professor','Prüfer','Redakteur','Referent','Richter','Schauspieler','Schneider','Schäfer','Sekretär','Berufseinsteiger','Bewerber','Kollege','Architekt','Doktor','Erzieher','Verwaltungsfachwirt','Forscher','Pädagoge','verwaltungswirt','Fachagrarwirt','Planer','Werkstudent','Spezialist','Analyst','Psychotherapeut','Rettungsschwimmer','Systemintegrator','Helfer','Elektroniker','doktorand','bibliothekar','bezirksschornsteinfeger','Elektromonteur','Fahrer','Pfleger','Diakon','Berufsjäger','Galvaniseur','Aufsichtshauer','Recruiter','Tiergesundheitsaufseher','Schlosser','Programmierer','Praktiker','Sprecher','Binnenschiffer','Prozessberater','Mülllader','Maschinenbediener','Inspektor','Helfer','Seher','Tester','Führer','Agent','Operator','Winzer','Taucher','Betreuer','Werker','Verwalter','Nautiker','Wärter','Wirtschafter','Übersetzer','Trainer','Dolmetscher','Developer','Mechaniker','Beauftragter','Sanitäter','Arbeiter','Ermittler','Wart','Schreiber','Überwacher','Wissenschaftler','Dachdecker','Ausbilder','Reiniger','Beobachter','Vorsteher','Arbeitnehmer','dezernent','forstwirt','bootsmann','kontrolleur', 'Benutzer','Feuerwehrmann','Polizist']\n",
    "berufe_femininum = []\n",
    "berufe_gm_plural = []\n",
    "for beruf in berufe:\n",
    "    berufe_femininum.extend([beruf + 'in', beruf + 'innen'])\n",
    "    berufe_gm_plural.extend([beruf + 'en'])\n",
    "    if beruf.endswith('er'):\n",
    "       berufe_gm_plural.extend([beruf + 'n'])\n",
    "#erweiterte_berufe.extend(b)\n",
    "\n",
    "# Entfernen der ursprünglichen Berufsbezeichnungen und Sortierung der Liste\n",
    "berufe_femininum.extend(extra_femininum)\n",
    "berufe_femininum = sorted(set(berufe_femininum))\n",
    "berufe_gm_plural.extend(extra_gm_plural)\n",
    "berufe_gm_plural = sorted(set(berufe_gm_plural))\n",
    "berufe_gm_plural.remove(\"Warten\")\n",
    "berufe_gm_plural.remove(\"Meistern\")\n",
    "berufe_gm_plural.remove(\"Leitern\")\n",
    "berufe_gm_plural.remove(\"Malern\")\n",
    "berufe_gm_plural.remove(\"Maurern\")\n",
    "berufe_gm_plural.remove(\"Schauspielern\")\n",
    "berufe_gm_plural.remove(\"Schneidern\")\n",
    "job_titles = [title.lower() for title in job_titles]\n",
    "berufe_gm_plural = [title.lower() for title in berufe_gm_plural]\n",
    "berufe_femininum = [title.lower() for title in berufe_femininum]\n",
    "\n",
    "pronomen = [\"man\", \"jemand\", \"niemand\"]\n",
    "gender_markers = [\"*\", \"/\", \":\", \"_\", \"·\",\"(in)\", \"(innen)\",\" *\", \" /\", \" :\", \" _\", \" ·\", \"(inn)\",\"[in]\", \"[innen]\", \"[inn]\" ]\n",
    "wortende = [\".\", \" \", \"-\", \",\", \"?\", \":\",\"/\", \"*\"]\n",
    "mwd = ['(m,w,d)', '(m,d,w)', '(w,m,d)', '(w,d,m)', '(d,w,m)', '(d,m,w)', '(m, w, d)', '(m, d, w)', '(w, m, d)', '(w, d, m)', '(d, w, m)', '(d, m, w)', '(m / w / d)', '(m / d / w)', '(w / m / d)', '(w / d / m)', '(d / w / m)', '(d / m / w)', '(m/w/d)', '(m/d/w)', '(w/m/d)', '(w/d/m)', '(d/w/m)', '(d/m/w)', '(m/ w/ d)', '(m/ d/ w)', '(w/ m/ d)', '(w/ d/ m)', '(d/ w/ m)', '(d/ m/ w)', '(m.w.d.)', '(m.d.w.)', '(w.m.d.)', '(w.d.m.)', '(d.w.m.)', '(d.m.w.)', '(m-w-d-k.A.)', '(w/m/d/k.A.)', '(m/w/d/k.A.)', '(m/w/i)', '[m|w|d]', '(/w/m/d)', '(M/W/D)', '(M/w/d)', '(W/M/D)', '(W/M/I)', '(all genders)', '(all genders*)', '(alle Geschlechter)', '(d/w/m/x)', '(f,m,div)', '(f/m/d)', '(f/m/div)', '(f/m/non-binary)', '(f/m/x)', '(jew. m/w/d)', '(jeweils m/w/d)', '(m,w,d,)', '(m,w,div.)', '(m,w.d)', '(m-w-d)', '(m./w./d.)', '(m./w./div.)', '(m/divers/w)', '(m/f/d)', '(m/f/div)', '(m/f/x)', '(m/w/d/)', '(m/w/d/k. A.)', '(m/w/d/k.A)', '(m/w/d/u)', '(m/w/d/x)', '(m/w/div)', '(m/w/div.)', '(m/w/divers)', '(m/w/i/t)', '(m/w/w)', '(m/wd/)', '(m:w:d)', '(mIwId)', '(mw/d)', '(m|w|d)', '(männlich, weiblich, divers)', '(männlich/weiblich/divers)', '(w-m-d)', '(w/d)', '(w/d/m/x)', '(w/m/d )', '(w/m/d/x)', '(w/m/div)', '(w/m/diverse)', '(w/m/i)', '(w/md)', '(w/w/d)', '(w7m/d)', '(w_m_d)', '(weiblich/männlich/divers)', '[m/w/d]']\n",
    "\n",
    "english_words = [\" the \", \" and \", \" of \", \" to \", \" a \", \" in \", \" that \", \" is \", \" was \", \" he \", \" for \", \" it \", \" with \", \" as \", \" his \", \" on \", \" be \", \" at \", \" by \"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "jobs_gr_1000_v6 als Startpunkt für Analyse GM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove hyphens from words\n",
    "def remove_hyphens_from_words(text):\n",
    "    return re.sub(r'(?<!\\s)-(?!\\s)', '', text)\n",
    "\n",
    "# Read the correctly formatted JSON file, V6 als Startpunkt für GM\n",
    "with open('jobs_gr_1000_v6.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Process each entry in the JSON file\n",
    "for entry in data:\n",
    "    entry['textfeld'] = remove_hyphens_from_words(entry['textfeld'])\n",
    "\n",
    "# Path for the new file\n",
    "new_file_path = 'jobs_gr_1000_v7.json'\n",
    "\n",
    "# Write the processed data to a new JSON file\n",
    "with open(new_file_path, 'w', encoding='utf-8') as new_file:\n",
    "    json.dump(data, new_file, indent=4, ensure_ascii=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "depricated, finden aller gen. maskulina, aber nicht nützlich, da sonst eine komplexere Wortanalyse notwendig wäre. Deshalb Einschränkung auf Substantive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Listen für Pronomen, Substantive und Endungen\n",
    "#pronomen = [\" jeder \", \" man \", \" jemand \", \" wer \", \" der \", \" niemand \", \" keiner \", \" Jeder \", \" Man \", \" Jemand \", \" Wer \", \" Der \", \" Niemand \", \" Keiner \",\"einen\", \" einen \"]\n",
    "substantive = [\"Arzt\", \"Lehrer\", \"Richter\", \"Koch\", \"Kollege\", \"Kunde\", \"Gast\", \"Autor\", \"Berater\"]\n",
    "endungen = [\"er\", \"en\", \"ant\", \"ist\", \"ent\", \"or\", \"ling\"]\n",
    "#wortende = [\".\", \" \", \"-\",\",\", \"?\"]\n",
    "\n",
    "def finde_wörter_mit_endungen(text, endungsliste):\n",
    "    gefundene_wörter = set()\n",
    "    for endung in endungsliste:\n",
    "        gefundene_wörter.update(re.findall(r'\\b\\w*' + endung + r'\\b', text))\n",
    "    return gefundene_wörter\n",
    "\n",
    "# JSON-Datei laden\n",
    "with open('jobs_gr_1000_v7.json', 'r', encoding='utf-8') as file:\n",
    "    jobs = json.load(file)\n",
    "\n",
    "# Durchgehen der Jobs und Ermitteln der Wörter\n",
    "for job in jobs:\n",
    "    text = job[\"title\"].lower()\n",
    "    gesuchte_wörter = set(substantive)  # Kombiniere Listen und entferne Duplikate\n",
    "    gesuchte_wörter.update(finde_wörter_mit_endungen(text, endungen))\n",
    "\n",
    "    ergebnis = Counter({wort: text.count(wort) for wort in gesuchte_wörter if wort in text})\n",
    "    job[\"gen_mask\"] = dict(ergebnis)\n",
    "\n",
    "# Speichern der aktualisierten JSON-Struktur\n",
    "with open('jobs_gr_1000_v8.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(jobs, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finden generischer Substantive. Simpel gehalten mithilfe der Listen unten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "# Listen für Pronomen, Substantive und Endungen\n",
    "\n",
    "\n",
    "def finde_substantive(text, substantive, endungen, wortende):\n",
    "    gefundene_substantive = set()\n",
    "\n",
    "    # Finde Wörter, die mit Großbuchstaben beginnen und in der Liste der Substantive stehen\n",
    "    for wort in substantive:\n",
    "        gefundene_substantive.update(re.findall(r'\\b[A-Z]' + wort + r'\\b(?=[{}])'.format(\"\".join(wortende)), text))\n",
    "\n",
    "    # Finde Wörter, die mit einer der Endungen enden\n",
    "    for endung in endungen:\n",
    "        gefundene_substantive.update(re.findall(r'\\b[A-Z]\\w*' + endung + r'\\b(?=[{}])'.format(\"\".join(wortende)), text))\n",
    "\n",
    "    return gefundene_substantive\n",
    "\n",
    "# JSON-Datei laden\n",
    "with open('jobs_gr_1000_v7.json', 'r', encoding='utf-8') as file:\n",
    "    jobs = json.load(file)\n",
    "\n",
    "# Durchgehen der Jobs und Ermitteln der Substantive\n",
    "for job in jobs:\n",
    "    text = job[\"title\"]\n",
    "    gesuchte_substantive = finde_substantive(text, substantive, endungen, wortende)\n",
    "\n",
    "    ergebnis = Counter({wort: text.count(wort) for wort in gesuchte_substantive})\n",
    "    job[\"gen_mask\"] = dict(ergebnis)\n",
    "\n",
    "# Speichern der aktualisierten JSON-Struktur\n",
    "with open('jobs_gr_1000_v8.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(jobs, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zählen der generischen Substantive. Substantive, die mindesten 10 mal im gesamten Datenbestant vorkommen, werden mit einberechnet. \n",
    "noch den Prozentsatz einbeziehen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('jobs_gr_1000_v8.json', 'r', encoding='utf-8') as file:\n",
    "    jobs = json.load(file)\n",
    "\n",
    "# Counter für die Häufigkeiten aller Wörter in gen_mask erstellen\n",
    "gesamt_häufigkeiten = Counter()\n",
    "\n",
    "for job in jobs:\n",
    "    if \"gen_mask\" in job:\n",
    "        gesamt_häufigkeiten.update(job[\"gen_mask\"])\n",
    "\n",
    "# Filtern der Wörter, die mindestens 10 Mal vorkommen\n",
    "häufige_wörter = {wort: häufigkeit for wort, häufigkeit in gesamt_häufigkeiten.items() if häufigkeit >= 3}\n",
    "\n",
    "# JSON-Datei mit den gefilterten Wörtern erstellen\n",
    "with open('substantive.json', 'w', encoding='utf-8') as json_file:\n",
    "    json.dump(häufige_wörter, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Ausgabe der sortierten Liste\n",
    "for wort, häufigkeit in sorted(häufige_wörter.items(), key=lambda item: item[1], reverse=True):\n",
    "    print(f\"{wort}: {häufigkeit}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('substantive.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Summieren der Werte\n",
    "total_sum = sum(data.values())\n",
    "\n",
    "print(total_sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Substantive, die von Chat GPT als Berufsbezeichnung ermittelt wurden. Diese decken für die Stellenbeschreibung ~2465 Artikel nicht ab. Alle anderen enthalten mindestens einen Eintrag aus der Liste. Stichprobenartige Untersuchung hat gezeigt, dass die fehlenden Einträge keinen konkreten Stellentitel vorweisen, dieser ist jedoch im Titel tendenziell ersichtlich. Da mit dieser Liste rund 80% aller Stellenausschreiben einbegriffen werden, ist sie auch solide genug....\n",
    "\n",
    "Prompt: \"welche dieser begriffe sind stellentitel aus stellenanzeigen, z. B. Sachbearbeiter: \"\n",
    "\n",
    "Anschließend Wörter zählen, die als Personenbezeichnung gezählt werden.\n",
    "\n",
    "Logik später angepasst: Liste von Job_titles wurde vom titel genommen und über regex wird nach Wortenden wieder geschaut, Paar Wörter weggenommen, da diese zu generisch sind oder auf en Enden zu schwer auswertbar sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_job_titles(text, titles, endings):\n",
    "    count_dict = Counter()\n",
    "    text_lower = text.lower()  # Umwandeln des Textes in Kleinbuchstaben\n",
    "    for title in titles:\n",
    "        title_lower = title.lower()  # Umwandeln der Berufsbezeichnungen in Kleinbuchstaben\n",
    "        # Regex, um das Wort als eigenständiges Wort oder am Ende eines anderen Wortes zu erkennen\n",
    "        pattern = rf\"(\\b{title_lower}\\b)|(\\w+{title_lower}(?={'|'.join(map(re.escape, endings))}))\"\n",
    "        count_dict[title] = len(re.findall(pattern, text_lower))\n",
    "    return count_dict\n",
    "\n",
    "def count_pronouns(text, pronouns, endings):\n",
    "    count_dict = Counter()\n",
    "    text_lower = text.lower()  # Umwandeln des Textes in Kleinbuchstaben\n",
    "    for pronoun in pronouns:\n",
    "        pronoun_lower = pronoun.lower()  # Umwandeln der Pronomen in Kleinbuchstaben\n",
    "        pattern = rf\"\\b{pronoun_lower}(?={'|'.join(map(re.escape, endings))})\"\n",
    "        count_dict[pronoun] = len(re.findall(pattern, text_lower))\n",
    "    return count_dict\n",
    "\n",
    "# JSON-Datei einlesen\n",
    "file_path = 'jobs_gr_1000_v7.json'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    jobs_data = json.load(file)\n",
    "\n",
    "# Jeden Eintrag im JSON verarbeiten\n",
    "for job in jobs_data:\n",
    "    text = job[\"textfeld\"]\n",
    "    job_counts = count_job_titles(text, job_titles, wortende)\n",
    "    pronoun_counts = count_pronouns(text, pronomen, wortende)\n",
    "    job[\"job_title_counts\"] = {title: count for title, count in job_counts.items() if count > 0}\n",
    "    job[\"pronoun_counts\"] = {pronoun: count for pronoun, count in pronoun_counts.items() if count > 0}\n",
    "\n",
    "# Die aktualisierten Daten in einer neuen Datei speichern\n",
    "updated_file_path_with_pronouns = 'jobs_gr_1000_v9_v2.json'\n",
    "with open(updated_file_path_with_pronouns, 'w', encoding='utf-8') as new_file:\n",
    "    json.dump(jobs_data, new_file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse/Prüfen der Stellenanzeigen ohne Eintrag in job_title_counts\n",
    "zuerst 3200 Einträge ohne Findungen, \n",
    "Nach Anpassung 1860"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyzing the 'job_title_counts' field in each entry of the JSON file and finding entries with no job titles\n",
    "\n",
    "# Read the updated JSON file\n",
    "with open('jobs_gr_1000_v9_v2.json', 'r', encoding='utf-8') as file:\n",
    "    updated_jobs_data = json.load(file)\n",
    "\n",
    "# Finding entries with no job title counts\n",
    "entries_with_no_job_titles = [job for job in updated_jobs_data if not job[\"job_title_counts\"]]\n",
    "num_entries_with_no_job_titles = sum(1 for job in updated_jobs_data if not job[\"job_title_counts\"])\n",
    "\n",
    "print(num_entries_with_no_job_titles)\n",
    "entries_with_no_job_titles\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finden aller Einträge, die nur die generische Jobbezeichnung haben, aber keine Beidnennung, Binnenmajuskel, Genderstern, Schrägstriche, Doppelpunkt, Einzelpunkt\n",
    "(2184)\n",
    "Regeln: \n",
    "1 - Wörter die in job_title_counts enthalten sind und gendergerecht IM Wort haben\n",
    "2 - Beidnennungen, geprüft über Finden des Wortstammes. Dafür soll über RegEx geschaut werden, ob der Wortstamm innerhalb 2 Wörter vor oder nach dem Finden von job_title_counts noch mal auftritt. Der Wortstamm wird definiert als die ersten vier Buchstaben vom Eintrag in job_title_counts. Umlaute werden gemapped, um Sonderfälle wie Ärztinnen abzudecken\n",
    "3 - Finden der Endungen aus Femininum innerhalb von drei Wörtern (nur) nach dem Finden von job_title_counts über RegEx. \n",
    "4 - Zählen von Verwendungen mwd innerhalb von 3 Wörtern von job_title_counts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_umlauts(text):\n",
    "    umlaut_mapping = {\n",
    "        'ä': 'a',\n",
    "        'ö': 'o',\n",
    "        'ü': 'u',\n",
    "        'Ä': 'A',\n",
    "        'Ö': 'O',\n",
    "        'Ü': 'U'\n",
    "    }\n",
    "    for umlaut, replacement in umlaut_mapping.items():\n",
    "        text = text.replace(umlaut, replacement)\n",
    "    return text\n",
    "# Function to check for bidennung\n",
    "# Function to check for bidennung\n",
    "def has_beidennung(text, job_titles, word_index):\n",
    "    #Entfernen der Umlaufe\n",
    "    text = replace_umlauts(text)\n",
    "    words = text.split()\n",
    "    # Ermittle das Wort am gegebenen Index und seinen Wortstamm\n",
    "    word_at_index = words[word_index]\n",
    "    word_stem = word_at_index[:5].lower()\n",
    "    # Ermittle die Startposition des Wortes am gegebenen Index\n",
    "    start_char_index_of_word = len(' '.join(words[:word_index])) + (1 if word_index > 0 else 0) \n",
    "    # Definiere den Bereich, in dem nach Duplikaten gesucht wird\n",
    "    start_index = max(0, start_char_index_of_word - 30)\n",
    "    end_index = min(len(text), start_char_index_of_word + len(word_at_index) + 30)\n",
    "    # Extrahiere den Textabschnitt innerhalb des Bereichs\n",
    "    text_section = text[start_index:end_index].lower()\n",
    "    # Zähle die Vorkommen des Wortstamms\n",
    "    count_duplicates = text_section.count(word_stem)\n",
    "    return 1 if count_duplicates >= 2 else 0\n",
    "\n",
    "# Function to check for a gender marker\n",
    "def has_gender_marker(word):\n",
    "    if any(marker in word for marker in gender_markers):\n",
    "        return 1  # True - Gendermarker gefunden\n",
    "    return 0\n",
    "\n",
    "# Function to check for the presence of mwd\n",
    "def count_mwd(text):\n",
    "    count = 0\n",
    "    for pattern in mwd:\n",
    "        count += text.count(pattern)\n",
    "    return count\n",
    "\n",
    "# Load the original JSON data\n",
    "file_path = 'jobs_gr_1000_v9_v2.json'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Update the data with the new fields\n",
    "for entry in data:\n",
    "    text = entry.get(\"textfeld\", \"\").lower()  # Umwandlung in Kleinbuchstaben\n",
    "    words = text.split()\n",
    "    mwd_count = count_mwd(text)\n",
    "    job_bezeichnung_results = {}\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        for title in job_titles:\n",
    "            title_lower = title.lower()\n",
    "            pattern = rf\"{title_lower}\\b|\\w+{title_lower}\\b\"\n",
    "            if re.search(pattern, word):\n",
    "\n",
    "                gender_marker = has_gender_marker(word)\n",
    "                beidennung = has_beidennung(text, title_lower, i)\n",
    "\n",
    "                if title not in job_bezeichnung_results:\n",
    "                    job_bezeichnung_results[title] = {\n",
    "                        \"job_title_results\": [],\n",
    "                        \"context_results\": [],\n",
    "                        \"gender_marker_results\": [],\n",
    "                        \"beidnennung_results\": []\n",
    "                    }\n",
    "\n",
    "                job_bezeichnung_results[title][\"job_title_results\"].append(title)\n",
    "                job_bezeichnung_results[title][\"context_results\"].append(\" \".join(words[max(0, i-5):i+6]))\n",
    "                job_bezeichnung_results[title][\"gender_marker_results\"].append(gender_marker)\n",
    "                job_bezeichnung_results[title][\"beidnennung_results\"].append(beidennung)\n",
    "\n",
    "    entry['job_bezeichnung'] = job_bezeichnung_results\n",
    "    entry['mwd_count'] = mwd_count\n",
    "\n",
    "# Save the updated data with the new structure to a new JSON file\n",
    "output_file_v10_structured = 'jobs_gr_1000_v10_v1_1.json'\n",
    "with open(output_file_v10_structured, 'w', encoding='utf-8') as file:\n",
    "    json.dump(data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "erweitern um femininum und gm_plural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'jobs_gr_1000_v10_v1_1.json'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Funktion, um das Textfeld zu durchsuchen\n",
    "def search_textfeld(textfeld, word_list, exclude_binnenmajuskel=False, include_binnenmajuskel=False):\n",
    "    found_words = []\n",
    "    found_binnenmajuskel = 0\n",
    "    for word in word_list:\n",
    "        pattern = rf\"\\b{word.lower()}\\b|\\b\\w+{word.lower()}\\b\"\n",
    "        matches = re.finditer(pattern, textfeld, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            match_word = textfeld[match.start():match.end()]\n",
    "            # Überprüfen auf Binnenmajuskeln\n",
    "            if any(c.isupper() for c in match_word[1:]):\n",
    "                found_binnenmajuskel +=1\n",
    "            else:\n",
    "                if exclude_binnenmajuskel and include_binnenmajuskel:\n",
    "                    continue\n",
    "                found_words.append(match_word)\n",
    "\n",
    "    if include_binnenmajuskel:\n",
    "        return found_binnenmajuskel\n",
    "    else:\n",
    "        return found_words\n",
    "\n",
    "# Bearbeitung jedes Eintrags in der JSON-Datei\n",
    "for entry in data:\n",
    "    textfeld = entry.get('textfeld', '')\n",
    "\n",
    "    # Suche nach Wörtern in den Listen, Binnenmajuskeln berücksichtigen oder ausschließen\n",
    "    entry['textfeld_gm_plural'] = search_textfeld(textfeld, berufe_gm_plural)\n",
    "    entry['textfeld_femininum'] = search_textfeld(textfeld, berufe_femininum, exclude_binnenmajuskel=True)\n",
    "    entry['text_binnenmajuskel'] = search_textfeld(textfeld, berufe_femininum, include_binnenmajuskel=True)\n",
    "\n",
    "\n",
    "# Speichern der bearbeiteten Daten in einer neuen Datei\n",
    "updated_file_path = 'jobs_gr_1000_v10_v2.json'\n",
    "with open(updated_file_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "löschen von duplikaten, 1714 \n",
    "löschen von herr/frau ..., 89"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_entries_with_counters(data):\n",
    "    duplicate_count = 0\n",
    "    herr_frau_count = 0\n",
    "\n",
    "    for entry in data:\n",
    "        job_titles_to_remove = set()\n",
    "\n",
    "        for job_title, job_title_data in entry.get(\"job_bezeichnung\", {}).items():\n",
    "            context_results = job_title_data[\"context_results\"]\n",
    "            gender_marker_results = job_title_data[\"gender_marker_results\"]\n",
    "            job_title_results = job_title_data[\"job_title_results\"]\n",
    "            beidnennung_results = job_title_data[\"beidnennung_results\"]\n",
    "\n",
    "            new_context_results = []\n",
    "            new_gender_marker_results = []\n",
    "            new_job_title_results = []\n",
    "            new_beidnennung_results = []\n",
    "\n",
    "            for i, result in enumerate(context_results):\n",
    "                is_duplicate = False\n",
    "                for unique_result in new_context_results:\n",
    "                    common_words = set(result.split()).intersection(set(unique_result.split()))\n",
    "                    if len(common_words) >= 6:\n",
    "                        is_duplicate = True\n",
    "                        duplicate_count += 1\n",
    "                        break\n",
    "\n",
    "                result_lower = result.lower()  # Konvertierung in Kleinbuchstaben\n",
    "                herr_frau_combination_found = any(\n",
    "                    title.lower() in result_lower for title in [f\"Herr {job_title}\", f\"Frau {job_title}\"]\n",
    "                )\n",
    "\n",
    "                if herr_frau_combination_found:\n",
    "                    job_titles_to_remove.add(job_title)\n",
    "                    herr_frau_count += 1\n",
    "                    break\n",
    "                elif not is_duplicate:\n",
    "                    new_context_results.append(result)\n",
    "                    new_gender_marker_results.append(gender_marker_results[i])\n",
    "                    new_job_title_results.append(job_title_results[i])\n",
    "                    new_beidnennung_results.append(beidnennung_results[i])\n",
    "\n",
    "            if job_title not in job_titles_to_remove:\n",
    "                job_title_data[\"context_results\"] = new_context_results\n",
    "                job_title_data[\"gender_marker_results\"] = new_gender_marker_results\n",
    "                job_title_data[\"job_title_results\"] = new_job_title_results\n",
    "                job_title_data[\"beidnennung_results\"] = new_beidnennung_results\n",
    "\n",
    "        for job_title in job_titles_to_remove:\n",
    "            del entry[\"job_bezeichnung\"][job_title]\n",
    "\n",
    "    return data, duplicate_count, herr_frau_count\n",
    "\n",
    "file_path = 'jobs_gr_1000_v10_v2.json'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "modified_data, duplicate_count, herr_frau_count = remove_entries_with_counters(data)\n",
    "\n",
    "# Output the counts\n",
    "print(duplicate_count, herr_frau_count)\n",
    "\n",
    "# Write the modified data back to a new file\n",
    "output_file_path_updated = 'jobs_gr_1000_v10_v2_updated.json'\n",
    "with open(output_file_path_updated, 'w', encoding='utf-8') as file:\n",
    "    json.dump(modified_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "output_file_path_updated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "finden aller einträge, wo kein job_title_counts, aber dennoch stellenausschreiben vorliegt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_data(data):\n",
    "    # Filter criteria\n",
    "    filtered_data = [\n",
    "        entry for entry in data \n",
    "        if not entry[\"job_title_counts\"] and not entry[\"pronoun_counts\"] and not entry[\"job_bezeichnung\"]\n",
    "    ]\n",
    "    return filtered_data\n",
    "\n",
    "# Load the original JSON data\n",
    "file_path = 'jobs_gr_1000_v10_v2_updated.json'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Apply the filter function\n",
    "filtered_data = filter_data(data)\n",
    "\n",
    "# Save the filtered data to a new file\n",
    "output_file_path = 'jobs_gr_1000_v10_v2_null.json'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(filtered_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "output_file_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_words_with_gender_markers_in_filtered_data(file_path, gender_markers):\n",
    "    # Load the filtered data\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        filtered_data = json.load(file)\n",
    "\n",
    "    # Regular expression to match words with specified gender markers\n",
    "    pattern = re.compile(r'\\b\\w*(?:' + '|'.join(map(re.escape, gender_markers)) + r')\\b')\n",
    "\n",
    "    # Search for matches in 'textfeld' of each entry\n",
    "    words_with_markers = []\n",
    "    for entry in filtered_data:\n",
    "        text = entry.get(\"textfeld\", \"\")\n",
    "        matches = pattern.findall(text)\n",
    "        words_with_markers.extend(matches)\n",
    "\n",
    "    # Count occurrences of each word\n",
    "    word_counts = {}\n",
    "    for word in words_with_markers:\n",
    "        word_counts[word] = word_counts.get(word, 0) + 1\n",
    "\n",
    "    # Sort words by frequency\n",
    "    sorted_words = sorted(word_counts.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_words\n",
    "\n",
    "# Path to the filtered data file\n",
    "filtered_file_path = 'jobs_gr_1000_v10_v2_null.json'\n",
    "\n",
    "# Define gender markers\n",
    "gender_markers = [\"*\", \"/\", \":\", \"_\", \"·\", \"I\"]\n",
    "\n",
    "# Apply the function to count words with gender markers\n",
    "words_with_gender_markers = find_words_with_gender_markers_in_filtered_data(filtered_file_path, gender_markers)\n",
    "\n",
    "# Output the sorted words\n",
    "words_with_gender_markers  \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
