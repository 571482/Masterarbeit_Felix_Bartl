{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse der Titel,\n",
    "ähnlich wie Analyse Textfeld\n",
    "Erst mal alle generischen Titel entnommen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_titles=['Administrator','Apotheker','Arzt','Assistent','Techniker','Zeichner','Beamter','Berater','Betriebswirt','Ologe','Buchhalter','Chemiker','Controller','Designer','Direktor','Disponent','Elektriker','Entwickler','Fotograf','Friseur','Geomatiker','Gärtner','Informatiker','Ingenieur','Journalist','Jurist','Kaufmann','Koch','Koordinator','Laborant','Lehrer','Leiter','Maler','Manager','Maschinist','Mathematiker','Maurer','Mechatroniker','Mediengestalter','Meister','Musiker','fachmann','Physiker','Professor','Prüfer','anwalt','Redakteur','Referent','Richter','Schauspieler','Schneider','Schäfer','Sekretär','Bauer','Berufseinsteiger','Bewerber','Kollege','Architekt','Doktor','Erzieher','Verwaltungsfachwirt','Forscher','Pädagoge','Diplomverwaltungswirt','Fachagrarwirt','Planer','Werkstudent','Spezialist','Analyst','Psychotherapeut','Rettungsschwimmer','Systemintegrator','Amtsbote','Helfer','Elektroniker','doktorand','bibliothekar','bezirksschornsteinfeger','Elektromonteur','Fahrer','Pfleger','Diakon','Controllingexperte','Berufsjäger','Galvaniseur','Aufsichtshauer','Kämmerer','Recruiter','Tiergesundheitsaufseher','Schlosser','Programmierer','Angestellter','Praktiker','Sprecher','Binnenschiffer','Prozessberater','Mülllader','Maschinenbediener','Inspektor','Helfer','Seher','Tester','Führer','Agent','Zimmerer','Operator','Beschäftigter','Winzer','Taucher','Betreuer','Werker','Verwalter','Nautiker','Wärter','Wirtschafter','Übersetzer','Trainer','Dolmetscher','Developer','Mechaniker','Beauftragter','Sanitäter','Arbeiter','Ermittler','Wart','Schreiber','Überwacher','Wissenschaftler','Dachdecker','Ausbilder','Verantwortlicher','Beigeordneter','Reiniger','Beobachter','Vorsteher','Arbeitnehmer','dezernent','forstwirt','kontrolleur', 'Benutzer', 'Feuerwehrmann','Polizist']\n",
    "\n",
    "extra_femininum=['angestelltin','assistenz','beamtin','beamtinnen','ingenieurinnen','meisterin','meisterinnen','Ärztin','Ärztinnen','ologin','ologinnen','Köchin','Köchinnen','kauffrau','friseuse','fachfrau','anwältin','anwältinnen','bäuerin','amtsbotin','amtsbotinnen','Controllingexpertin','Controllingexpertinnen','kämmerin','kämmerinnen','zimmerin','zimmerinnen','Beauftragtin','Beauftragtinnen','beigeordneterin','beigeordneterinnen','pädagogin','pädagoginnen','bootsfrau', 'Feuerwehrfrau']\n",
    "\n",
    "extra_gm_plural=['angestellte','angestellten','beamte','ingenieure','Ärzte','ologen','Köche','anwälte','amtsboten','Controllingexperten','beschäftigte','Beauftragte','beigeordnete','verantwortliche','pädagogen','kolleg','Controllingexpert','Amtsbot','Pädagog','ingenieuren','arbeitenden','reinigungskraft','beamten','lehrkraft','schreibkraft','fachkraft','hilfskraft','assistenzkraft']\n",
    "\n",
    "berufe=['Administrator','Apotheker','Assistent','Techniker','Zeichner','Berater','Betriebswirt','Buchhalter','Chemiker','Controller','friseur','Designer','Direktor','Disponent','Elektriker','Entwickler','Fotograf','Friseur','Geomatiker','Gärtner','Informatiker','Journalist','Jurist','Koch','Koordinator','Laborant','Lehrer','Leiter','Maler','Manager','Maschinist','Mathematiker','Maurer','Mechatroniker','Mediengestalter','Meister','Musiker','Physiker','Professor','Prüfer','Redakteur','Referent','Richter','Schauspieler','Schneider','Schäfer','Sekretär','Berufseinsteiger','Bewerber','Kollege','Architekt','Doktor','Erzieher','Verwaltungsfachwirt','Forscher','Pädagoge','verwaltungswirt','Fachagrarwirt','Planer','Werkstudent','Spezialist','Analyst','Psychotherapeut','Rettungsschwimmer','Systemintegrator','Helfer','Elektroniker','doktorand','bibliothekar','bezirksschornsteinfeger','Elektromonteur','Fahrer','Pfleger','Diakon','Berufsjäger','Galvaniseur','Aufsichtshauer','Recruiter','Tiergesundheitsaufseher','Schlosser','Programmierer','Praktiker','Sprecher','Binnenschiffer','Prozessberater','Mülllader','Maschinenbediener','Inspektor','Helfer','Seher','Tester','Führer','Agent','Operator','Winzer','Taucher','Betreuer','Werker','Verwalter','Nautiker','Wärter','Wirtschafter','Übersetzer','Trainer','Dolmetscher','Developer','Mechaniker','Beauftragter','Sanitäter','Arbeiter','Ermittler','Wart','Schreiber','Überwacher','Wissenschaftler','Dachdecker','Ausbilder','Reiniger','Beobachter','Vorsteher','Arbeitnehmer','dezernent','forstwirt','bootsmann','kontrolleur', 'Benutzer','Feuerwehrmann','Polizist']\n",
    "berufe_femininum = []\n",
    "berufe_gm_plural = []\n",
    "for beruf in berufe:\n",
    "    berufe_femininum.extend([beruf + 'in', beruf + 'innen'])\n",
    "    berufe_gm_plural.extend([beruf + 'en'])\n",
    "    if beruf.endswith('er'):\n",
    "       berufe_gm_plural.extend([beruf + 'n'])\n",
    "#erweiterte_berufe.extend(b)\n",
    "\n",
    "# Entfernen der ursprünglichen Berufsbezeichnungen und Sortierung der Liste\n",
    "berufe_femininum.extend(extra_femininum)\n",
    "berufe_femininum = sorted(set(berufe_femininum))\n",
    "berufe_gm_plural.extend(extra_gm_plural)\n",
    "berufe_gm_plural = sorted(set(berufe_gm_plural))\n",
    "berufe_gm_plural.remove(\"Warten\")\n",
    "berufe_gm_plural.remove(\"Meistern\")\n",
    "berufe_gm_plural.remove(\"Leitern\")\n",
    "berufe_gm_plural.remove(\"Malern\")\n",
    "berufe_gm_plural.remove(\"Maurern\")\n",
    "berufe_gm_plural.remove(\"Schauspielern\")\n",
    "berufe_gm_plural.remove(\"Schneidern\")\n",
    "job_titles = [title.lower() for title in job_titles]\n",
    "berufe_gm_plural = [title.lower() for title in berufe_gm_plural]\n",
    "berufe_femininum = [title.lower() for title in berufe_femininum]\n",
    "\n",
    "pronomen = [\"man\", \"jemand\", \"niemand\"]\n",
    "gender_markers = [\"*\", \"/\", \":\", \"_\", \"·\",\"(in)\", \"(innen)\",\" *\", \" /\", \" :\", \" _\", \" ·\", \"(inn)\",\"[in]\", \"[innen]\", \"[inn]\" ]\n",
    "wortende = [\".\", \" \", \"-\", \",\", \"?\", \":\",\"/\", \"*\"]\n",
    "mwd = ['(m,w,d)', '(m,d,w)', '(w,m,d)', '(w,d,m)', '(d,w,m)', '(d,m,w)', '(m, w, d)', '(m, d, w)', '(w, m, d)', '(w, d, m)', '(d, w, m)', '(d, m, w)', '(m / w / d)', '(m / d / w)', '(w / m / d)', '(w / d / m)', '(d / w / m)', '(d / m / w)', '(m/w/d)', '(m/d/w)', '(w/m/d)', '(w/d/m)', '(d/w/m)', '(d/m/w)', '(m/ w/ d)', '(m/ d/ w)', '(w/ m/ d)', '(w/ d/ m)', '(d/ w/ m)', '(d/ m/ w)', '(m.w.d.)', '(m.d.w.)', '(w.m.d.)', '(w.d.m.)', '(d.w.m.)', '(d.m.w.)', '(m-w-d-k.A.)', '(w/m/d/k.A.)', '(m/w/d/k.A.)', '(m/w/i)', '[m|w|d]', '(/w/m/d)', '(M/W/D)', '(M/w/d)', '(W/M/D)', '(W/M/I)', '(all genders)', '(all genders*)', '(alle Geschlechter)', '(d/w/m/x)', '(f,m,div)', '(f/m/d)', '(f/m/div)', '(f/m/non-binary)', '(f/m/x)', '(jew. m/w/d)', '(jeweils m/w/d)', '(m,w,d,)', '(m,w,div.)', '(m,w.d)', '(m-w-d)', '(m./w./d.)', '(m./w./div.)', '(m/divers/w)', '(m/f/d)', '(m/f/div)', '(m/f/x)', '(m/w/d/)', '(m/w/d/k. A.)', '(m/w/d/k.A)', '(m/w/d/u)', '(m/w/d/x)', '(m/w/div)', '(m/w/div.)', '(m/w/divers)', '(m/w/i/t)', '(m/w/w)', '(m/wd/)', '(m:w:d)', '(mIwId)', '(mw/d)', '(m|w|d)', '(männlich, weiblich, divers)', '(männlich/weiblich/divers)', '(w-m-d)', '(w/d)', '(w/d/m/x)', '(w/m/d )', '(w/m/d/x)', '(w/m/div)', '(w/m/diverse)', '(w/m/i)', '(w/md)', '(w/w/d)', '(w7m/d)', '(w_m_d)', '(weiblich/männlich/divers)', '[m/w/d]']\n",
    "\n",
    "english_words = [\" the \", \" and \", \" of \", \" to \", \" a \", \" in \", \" that \", \" is \", \" was \", \" he \", \" for \", \" it \", \" with \", \" as \", \" his \", \" on \", \" be \", \" at \", \" by \"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Title zu Titel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('jobs_gr_1000_v10_v2_updated.json', 'r', encoding='utf-8') as file:\n",
    "    jobs_data = json.load(file)\n",
    "\n",
    "# 'title' zu 'titel' ändern\n",
    "for job in jobs_data:\n",
    "    job['titel'] = job.pop('title')\n",
    "\n",
    "# Speichern der geänderten Daten\n",
    "with open('jobs_gr_1000_v10_v2_updated.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(jobs_data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_job_titles(text, titles):\n",
    "    count_dict = Counter()\n",
    "    text_lower = text.lower()\n",
    "    for title in titles:\n",
    "        # Konvertiere den Titel in Kleinbuchstaben für die Suche\n",
    "        title_lower = title.lower()\n",
    "        pattern = rf\"{title_lower}\"\n",
    "        count_dict[title] += len(re.findall(pattern, text_lower))\n",
    "    return count_dict\n",
    "\n",
    "def count_pronouns(text, pronouns, endings):\n",
    "    count_dict = Counter()\n",
    "    text_lower = text.lower()\n",
    "    for pronoun in pronouns:\n",
    "        # Beachte auch hier die Kleinbuchstaben\n",
    "        pronoun_lower = pronoun.lower()\n",
    "        pattern = rf\"\\b{pronoun_lower}(?={'|'.join(map(re.escape, endings))})\"\n",
    "        count_dict[pronoun] = len(re.findall(pattern, text_lower))\n",
    "    return count_dict\n",
    "\n",
    "file_path = 'jobs_gr_1000_v10_v2_updated.json'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    jobs_data = json.load(file)\n",
    "\n",
    "# Process each entry in the JSON\n",
    "for job in jobs_data:\n",
    "    text = job[\"titel\"].lower()\n",
    "    job_counts = count_job_titles(text, job_titles)\n",
    "    pronoun_counts = count_pronouns(text, pronomen, wortende)\n",
    "    job[\"title_job_counts\"] = {title: count for title, count in job_counts.items() if count > 0}\n",
    "    job[\"title_pronoun_counts\"] = {pronoun: count for pronoun, count in pronoun_counts.items() if count > 0}\n",
    "\n",
    "# Save the updated data in a new file\n",
    "updated_file_path_with_pronouns = 'jobs_titel_v1.json'\n",
    "with open(updated_file_path_with_pronouns, 'w', encoding='utf-8') as new_file:\n",
    "    json.dump(jobs_data, new_file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyse/Prüfen der Titel ohne Eintrag in job_title_counts\n",
    "6300 Einträge ohne Jobtitel, Prüfung über erstellen der häufigsten Wörter im Titel und nehmen aller, die 15 oder öfter vorkommen\n",
    "3298 nach anpassung von RegEx und iterativen anpassen\n",
    "871 ohne typische Bezeichnungen\n",
    "Rest hat typisch neutrale Bezeichnungen wie Kraft oder weibliche Form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    words = re.findall(r'\\b\\w{4,}\\b', text.lower())  # Only words with 4 or more characters\n",
    "    return Counter(words)\n",
    "\n",
    "# Read the JSON file\n",
    "file_path = 'jobs_titel_v1.json'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    jobs_data = json.load(file)\n",
    "\n",
    "word_counter = Counter()\n",
    "\n",
    "# Process each entry in the JSON\n",
    "for job in jobs_data:\n",
    "    if job[\"title_job_counts\"] == {}:\n",
    "        word_counter += count_words(job[\"titel\"])\n",
    "\n",
    "# Filter for words that occur at least 15 times\n",
    "words_min_15_occurrences = {word: count for word, count in word_counter.items() if count >= 15}\n",
    "\n",
    "# Write the words to a JSON file\n",
    "output_file = 'substantive2.json'\n",
    "with open(output_file, 'w', encoding='utf-8') as file:\n",
    "    json.dump(words_min_15_occurrences, file, indent=4, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Konkrete Titel um job_titels anzupassen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('substantive2.json', 'r', encoding='utf-8') as file:\n",
    "    substantive_words = json.load(file)\n",
    "    substantive_set = set(substantive_words.keys())  # Convert to a set for efficient searching\n",
    "\n",
    "# Function to check if any substantive word occurs in text\n",
    "def contains_substantive(text, substantive_set):\n",
    "    words = set(re.findall(r'\\b\\w+\\b', text.lower()))\n",
    "    return any(word in substantive_set for word in words)\n",
    "\n",
    "# Read the JSON file jobs_titel_v1.json\n",
    "with open('jobs_titel_v1.json', 'r', encoding='utf-8') as file:\n",
    "    jobs_data = json.load(file)\n",
    "\n",
    "titles_without_substantive = []\n",
    "\n",
    "# Process each entry in the JSON\n",
    "for job in jobs_data:\n",
    "    if job[\"title_job_counts\"] == {} and not contains_substantive(job[\"titel\"], substantive_set):\n",
    "        titles_without_substantive.append(job[\"titel\"])\n",
    "\n",
    "titles_without_substantive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gucken, wie oft etwas nicht in job_liste vorkommt\n",
    "häufigsten wörter ermitteln, grenze: 15 Vorkommnisse und Wort mindestens 4 Zeichen lang\n",
    "Ermitteln, wie oft Titel keines der häufigsten Wörter enthält\n",
    "3493 mal keine job_titel in titel\n",
    "884 mal sind wörter nicht aus Substantive enthalten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Erstellung von v2 nach der selben logik wie textfeld: \n",
    "has_beidennung, has_gender_marker, has_feminine_ending_nouns,is_duplicate_stem_nearby, count_mwd\n",
    "als vergleich werden wieder wörter aus job_titels genommen. RegEx hilft, und prüft Findungen + Findungen am Wortende (Liste von möglichen Wortenden ebenfalls iterativ ermittelt)\n",
    "Als Ergebnis neue Spalten wieder hinzugefügt, um Titel zusätzlich zu analysieren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'jobs_titel_v1.json'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "#Ersetzen der Umlaufe\n",
    "def replace_umlauts(text):\n",
    "    umlaut_mapping = {\n",
    "        'ä': 'a',\n",
    "        'ö': 'o',\n",
    "        'ü': 'u',\n",
    "        'Ä': 'A',\n",
    "        'Ö': 'O',\n",
    "        'Ü': 'U'\n",
    "    }\n",
    "    for umlaut, replacement in umlaut_mapping.items():\n",
    "        text = text.replace(umlaut, replacement)\n",
    "    return text\n",
    "\n",
    "# Function to check for bidennung\n",
    "def has_beidennung(text, job_titles, word_index):\n",
    "    text = replace_umlauts(text)\n",
    "    words = text.split()\n",
    "    # Ermittle das Wort am gegebenen Index und seinen Wortstamm\n",
    "    word_at_index = words[word_index]\n",
    "    word_stem = word_at_index[:5].lower()\n",
    "    # Ermittle die Startposition des Wortes am gegebenen Index\n",
    "    start_char_index_of_word = len(' '.join(words[:word_index])) + (1 if word_index > 0 else 0)   \n",
    "    # Definiere den Bereich, in dem nach Duplikaten gesucht wird\n",
    "    start_index = max(0, start_char_index_of_word - 40)\n",
    "    end_index = min(len(text), start_char_index_of_word + len(word_at_index) + 40)   \n",
    "    # Extrahiere den Textabschnitt innerhalb des Bereichs\n",
    "    text_section = text[start_index:end_index].lower()\n",
    "    # Zähle die Vorkommen des Wortstamms\n",
    "    count_duplicates = text_section.count(word_stem)\n",
    "    return 1 if count_duplicates >= 2 else 0\n",
    "\n",
    "# Function to check for a gender marker\n",
    "def has_gender_marker(word):\n",
    "    if any(marker in word for marker in gender_markers):\n",
    "        return 1  # True - Gendermarker gefunden\n",
    "    return 0\n",
    "\n",
    "# Function to check for the presence of mwd\n",
    "def count_mwd(text):\n",
    "    count = 0\n",
    "    for pattern in mwd:\n",
    "        count += text.count(pattern)\n",
    "    return count\n",
    "\n",
    "for entry in data:\n",
    "    text = entry.get(\"titel\", \"\").lower()  # Bezieht sich auf das Feld \"titel\"\n",
    "    words = text.split()\n",
    "    titel_mwd_count = count_mwd(text)  # Neues Feld für MWD-Zählung\n",
    "    titel_job_bezeichnung_results = {}  # Neues Feld für Jobbezeichnungen\n",
    "\n",
    "    for i, word in enumerate(words):\n",
    "        for title in job_titles:  # Verwendet die vorgegebene job_titles-Liste\n",
    "            title_lower = title.lower()\n",
    "            pattern = rf\"{title_lower}\\b|\\w+{title_lower}\\b\"\n",
    "            if re.search(pattern, word):\n",
    "\n",
    "                gender_marker = has_gender_marker(word)\n",
    "                beidennung = has_beidennung(text, job_titles, i)  # Verwendung der überarbeiteten Funktion\n",
    "\n",
    "                if title not in titel_job_bezeichnung_results:\n",
    "                    titel_job_bezeichnung_results[title] = {\n",
    "                        \"titel_job_title_results\": [],\n",
    "                        \"titel_context_results\": [],\n",
    "                        \"titel_gender_marker_results\": [],\n",
    "                        \"titel_beidnennung_results\": []\n",
    "                    }\n",
    "\n",
    "                titel_job_bezeichnung_results[title][\"titel_job_title_results\"].append(title)\n",
    "                titel_job_bezeichnung_results[title][\"titel_context_results\"].append(\" \".join(words[max(0, i-5):i+6]))\n",
    "                titel_job_bezeichnung_results[title][\"titel_gender_marker_results\"].append(gender_marker)\n",
    "                titel_job_bezeichnung_results[title][\"titel_beidnennung_results\"].append(beidennung)\n",
    "\n",
    "    entry['titel_job_bezeichnung'] = titel_job_bezeichnung_results  # Neues Feld im Eintrag\n",
    "    entry['titel_mwd_count'] = titel_mwd_count  # Neues Feld für MWD-Zählung\n",
    "\n",
    "# Save the updated data with the new structure to a new JSON file\n",
    "output_file_v10_structured = 'jobs_titel_v2.json'\n",
    "with open(output_file_v10_structured, 'w', encoding='utf-8') as file:\n",
    "    json.dump(data, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = 'jobs_titel_v2.json'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Funktion, um das Textfeld zu durchsuchen\n",
    "def search_textfeld(titel, word_list, exclude_binnenmajuskel=False, include_binnenmajuskel=False):\n",
    "    found_words = []\n",
    "    found_binnenmajuskel = 0\n",
    "    for word in word_list:\n",
    "        pattern = rf\"\\b{word.lower()}\\b|\\b\\w+{word.lower()}\\b\"\n",
    "        matches = re.finditer(pattern, titel, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            match_word = titel[match.start():match.end()]\n",
    "            # Überprüfen auf Binnenmajuskeln\n",
    "            if any(c.isupper() for c in match_word[1:]):\n",
    "                found_binnenmajuskel+=1\n",
    "            else:\n",
    "                if exclude_binnenmajuskel and include_binnenmajuskel:\n",
    "                    continue\n",
    "                found_words.append(match_word)\n",
    "\n",
    "    if include_binnenmajuskel:\n",
    "        return found_binnenmajuskel\n",
    "    else:\n",
    "        return found_words\n",
    "\n",
    "# Bearbeitung jedes Eintrags in der JSON-Datei\n",
    "for entry in data:\n",
    "    titel = entry.get('titel', '')\n",
    "\n",
    "    # Suche nach Wörtern in den Listen, Binnenmajuskeln berücksichtigen oder ausschließen\n",
    "    entry['titel_gm_plural'] = search_textfeld(titel, berufe_gm_plural)\n",
    "    entry['titel_femininum'] = search_textfeld(titel, berufe_femininum, exclude_binnenmajuskel=True)\n",
    "    entry['titel_binnenmajuskel'] = search_textfeld(titel, berufe_femininum, include_binnenmajuskel=True)\n",
    "# Speichern der bearbeiteten Daten in einer neuen Datei\n",
    "updated_file_path = 'jobs_titel_v2_updated.json'\n",
    "with open(updated_file_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "updated_file_path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANalyse aller 0 Einträge für title_job_counts und job_bezeichnung, 1281 Einträge\n",
    "Bisher nur für generische Maskulina gezählt, Genderformen müssten noch extra gezählt werden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_entries(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    filtered_data = [entry for entry in data if not entry.get(\"title_job_counts\") and not entry.get(\"job_bezeichnung\")]\n",
    "\n",
    "    return filtered_data\n",
    "\n",
    "# Dateipfad der ursprünglichen JSON-Datei\n",
    "input_file_path = 'jobs_titel_v2_updated.json'\n",
    "\n",
    "# Filtern der Einträge\n",
    "filtered_data = filter_entries(input_file_path)\n",
    "\n",
    "# Speichern der gefilterten Daten in einer neuen Datei\n",
    "output_file_path = 'jobs_titel_v3.json'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(filtered_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "output_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gendermarker im Text zu finden ist nutzlos, weil man nur sonstige Sonderzeichen findet. Somit werden keine Sonderzeichen stant jetzt gefunden außerhalb generischer maskulina. Ggf. Prüfung über Wortstamm aus job_titles und ob eines der Zeichen in der nähe ist. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_gender_markers_with_context(text, markers, mwd_patterns):\n",
    "    marker_contexts = []\n",
    "    for marker in markers:\n",
    "        pattern = rf\"(?<!\\s){re.escape(marker)}(?! )|(?<! ) {re.escape(marker)}(?! )\"\n",
    "        matches = re.finditer(pattern, text)\n",
    "\n",
    "        for match in matches:\n",
    "            start, end = match.span()\n",
    "            if not any(mwd in text[max(0, start-10):min(len(text), end+10)] for mwd in mwd_patterns):\n",
    "                context_start = max(0, start - 30)  # Ca. 6 Wörter vor dem Marker\n",
    "                context_end = min(len(text), end + 30)  # Ca. 6 Wörter nach dem Marker\n",
    "                context = text[context_start:context_end]\n",
    "                marker_contexts.append(context)\n",
    "\n",
    "    return len(marker_contexts), marker_contexts\n",
    "\n",
    "def count_beidnennungen_with_context(text, job_titles):\n",
    "    beidnennungen_contexts = []\n",
    "    words = text.split()\n",
    "    for i, word in enumerate(words):\n",
    "        word_stem = word[:4].lower()\n",
    "        for title in job_titles:\n",
    "            title_stem = title[:4].lower()\n",
    "            if word_stem == title_stem:\n",
    "                for j in range(max(0, i-5), min(len(words), i+6)):\n",
    "                    if i != j and words[j][:4].lower() == title_stem:\n",
    "                        context_start = max(0, i - 6)\n",
    "                        context_end = min(len(words), i + 7)\n",
    "                        context = \" \".join(words[context_start:context_end])\n",
    "                        beidnennungen_contexts.append(context)\n",
    "                        break\n",
    "    return len(beidnennungen_contexts), beidnennungen_contexts\n",
    "\n",
    "\n",
    "# JSON-Daten laden\n",
    "file_path = 'jobs_titel_v2_updated.json'\n",
    "with open(file_path, 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Analysiere jeden Eintrag\n",
    "for entry in data:\n",
    "    gender_marker_count, gender_marker_contexts = find_gender_markers_with_context(entry.get('textfeld', ''), gender_markers, mwd)\n",
    "    beidnennungen_count, beidnennungen_contexts = count_beidnennungen_with_context(entry.get('textfeld', ''), job_titles)\n",
    "\n",
    "    entry['gender_marker_all_text'] = gender_marker_count\n",
    "    entry['gender_marker_contexts_text'] = gender_marker_contexts\n",
    "    entry['beidnennung_all_text'] = beidnennungen_count\n",
    "    entry['beidnennung_contexts_text'] = beidnennungen_contexts\n",
    "\n",
    "# Aktualisierte Daten speichern\n",
    "output_file_path = 'jobs_titel_v4.json'\n",
    "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "    json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_english_words(text, word_list):\n",
    "    count = 0\n",
    "    for word in word_list:\n",
    "        if word in text:\n",
    "            count += 1\n",
    "    return count\n",
    "\n",
    "# Liste der englischen Wörter\n",
    "\n",
    "# Laden der JSON-Datei\n",
    "with open('jobs_titel_v2_updated.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Zählen und Entfernen von Einträgen\n",
    "removed_entries = []\n",
    "for entry in data:\n",
    "    if count_english_words(entry[\"textfeld\"].lower(), english_words) >= 7:\n",
    "        removed_entries.append(entry)\n",
    "\n",
    "# Entfernen der Einträge aus der ursprünglichen Liste\n",
    "for entry in removed_entries:\n",
    "    data.remove(entry)\n",
    "    print(f\"Textfeld des entfernten Eintrags: {entry['textfeld']}\")\n",
    "\n",
    "# Anzahl der entfernten Einträge\n",
    "removed_count = len(removed_entries)\n",
    "\n",
    "# Speichern der gefilterten Daten\n",
    "with open('jobs_titel_v5.json', 'w', encoding='utf-8') as file:\n",
    "    json.dump(data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "print(f\"{removed_count} Einträge wurden entfernt.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "letzte Anpassungen\n",
    "\n",
    "wieder iterativ verbesser\n",
    "\n",
    "Gendermarker sehr simpel gehalten\n",
    "\n",
    "Postprocessing um genaugkeit etwas zu erhöhen\n",
    "    spezielle substantive weg\n",
    "    genauere definitiion für gendermarker (\" / \")\n",
    "    exceptions für beidnennungen (schule als wortstamm, aber deshalb falsch)\n",
    "\n",
    "ggf. feminine ending ergänzen, funktioniert halt nicht\n",
    "dann analyse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('jobs_titel_v2.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "def extract_text_with_parentheses_and_brackets(title):\n",
    "    result = []\n",
    "    open_paren = [\"(\", \"[\"]\n",
    "    close_paren = [\")\", \"]\"]\n",
    "    stack = []\n",
    "    current_text = \"\"\n",
    "\n",
    "    for char in title:\n",
    "        if char in open_paren:\n",
    "            if len(stack) == 0:  # Only start a new segment if not already in a nested segment\n",
    "                current_text = char\n",
    "            stack.append(char)\n",
    "        elif char in close_paren:\n",
    "            if len(stack) > 0 and open_paren.index(stack[-1]) == close_paren.index(char):\n",
    "                stack.pop()\n",
    "                current_text += char\n",
    "                if len(stack) == 0:  # Only add the segment if it's not nested\n",
    "                    result.append(current_text)\n",
    "                    current_text = \"\"\n",
    "            elif len(stack) > 0:\n",
    "                current_text += char\n",
    "        elif len(stack) > 0:\n",
    "            current_text += char\n",
    "\n",
    "    return result\n",
    "\n",
    "# Extract text from titles with parentheses and brackets included\n",
    "extracted_texts_with_parentheses = [extract_text_with_parentheses_and_brackets(job['titel']) for job in data]\n",
    "\n",
    "# Find unique values\n",
    "unique_texts_with_parentheses = set()\n",
    "for texts in extracted_texts_with_parentheses:\n",
    "    for text in texts:\n",
    "        unique_texts_with_parentheses.add(text)\n",
    "\n",
    "print(list(sorted(unique_texts_with_parentheses)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nouns(title):\n",
    "    return re.findall(r'\\b[A-Z][a-z]*\\b', title)\n",
    "\n",
    "# Laden Sie Ihre JSON-Datei\n",
    "with open('jobs_titel_v5.json', 'r', encoding='utf-8') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "# Extrahieren Sie Substantive aus Titeln und zählen Sie ihre Häufigkeit\n",
    "nouns_in_titles = []\n",
    "for job in data:\n",
    "    nouns = find_nouns(job['titel'])\n",
    "    nouns_in_titles.extend(nouns)\n",
    "\n",
    "nouns_frequency = Counter(nouns_in_titles)\n",
    "\n",
    "# Sortieren Sie die Substantive nach ihrer Häufigkeit\n",
    "sorted_nouns_by_frequency = sorted(nouns_frequency.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Hier können Sie nun die sortierte Liste anzeigen oder verwenden\n",
    "for noun, frequency in sorted_nouns_by_frequency:\n",
    "    print(f\"{noun}: {frequency}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gesuchte_woerter = ['Administrator','Apotheker','Arzt','Assistent', 'Techniker','Zeichner','Beamter','Berater','Betriebswirt','Ologe','Buchhalter','Chemiker','Controller','Designer','Direktor','Disponent','Elektriker','Entwickler','Fotograf','Friseur','Geomatiker','Gärtner','Informatiker','Ingenieur','Journalist', 'Jurist','Kaufmann','Koch','Koordinator','Laborant','Lehrer','Leiter','Maler','Manager','Maschinist','Mathematiker','Maurer','Mechatroniker','Mediengestalter','Meister','Musiker','fachmann','Physiker','Professor','Prüfer','Rechtsanwalt','Redakteur','Referent','Richter','Schauspieler','Schneider','Schumacher','Schäfer','Sekretär','Bauer', \"Berufseinsteiger\", \"Bewerber\", \"Kollege\", \"Architekt\", \"Doktor\",   \"Erzieher\", \"Verwaltungsfachwirt\", \"Forscher\",   \"Pädagoge\",  \"Diplomverwaltungswirt\", \"Fachagrarwirt\",   \"Planer\", \"Werkstudent\", \"Spezialist\",  \"Analyst\", \"Psychotherapeut\",  \"Rettungsschwimmer\",  \"Systemintegrator\", \"Amtsbote\", \"Helfer\",  \"Elektroniker\", \"doktorand\", \"bibliothekar\",  \"bezirksschornsteinfeger\",\"Elektromonteur\",   \"Fahrer\",  \"Pfleger\",  \"Diakon\",      \"Controllingexperte\", \"Berufsjäger\",    \"Galvaniseur\",   \"Aufsichtshauer\", \"Kämmerer\", \"Recruiter\",    \"Tiergesundheitsaufseher\",   \"Schlosser\",   \"Programmierer\",  \"Angestellter\", \"Praktiker\", \"Sprecher\", \"Binnenschiffer\",   \"Prozessberater\",\"Mülllader\",\"Maschinenbediener\", \"Inspektor\", \"Helfer\", \"Seher\", \"Tester\", \"Führer\", \"Agent\", \"Zimmerer\", \"Operator\", \"Beschäftigter\", \"Winzer\", \"Taucher\", \"Betreuer\", \"Werker\", \"Verwalter\", \"Nautiker\", \"Wärter\", \"Wirtschafter\", \"Übersetzer\", \"Trainer\", \"Dolmetscher\", \"Developer\",\"Mechaniker\", \"Beauftragter\", \"Sanitäter\", \"Arbeiter\", \"Ermittler\", \"Wart\", \"Schreiber\", \"Überwacher\", \"Wissenschaftler\", \"User\", \"Dachdecker\", \"Ausbilder\", \"Verantwortlicher\", \"Beigeordneter\", \"Reiniger\", \"Beobachter\", \"Worker\",  \"Vorsteher\", \"Arbeitnehmer\"\n",
    "]\n",
    "\n",
    "gesuchte_woerter_klein = [word.lower() for word in gesuchte_woerter]\n",
    "\n",
    "total_sum = 0\n",
    "sum_at_least_10 = 0\n",
    "sum_gesuchte_woerter = 0\n",
    "gefundene_woerter = {}\n",
    "nicht_gefundene_woerter = {}\n",
    "sum_nicht_gefundene_woerter = 0\n",
    "\n",
    "with open('substantive_titel.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file:\n",
    "        parts = line.split(':')\n",
    "        if len(parts) == 2:\n",
    "            word, count = parts[0].strip().lower(), int(parts[1])\n",
    "            total_sum += count\n",
    "            if count >= 10:\n",
    "                sum_at_least_10 += count\n",
    "\n",
    "            if any(word.endswith(sub_word) for sub_word in gesuchte_woerter_klein):\n",
    "                sum_gesuchte_woerter += count\n",
    "                gefundene_woerter[word] = count\n",
    "            else:\n",
    "                nicht_gefundene_woerter[word] = count\n",
    "                sum_nicht_gefundene_woerter += count\n",
    "\n",
    "sortierte_nicht_gefundene_woerter = sorted(nicht_gefundene_woerter.items(), key=lambda x: x[1], reverse=True)\n",
    "sortierte_gefundene_woerter = sorted(gefundene_woerter.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Ausgabe der Ergebnisse\n",
    "print(f\"Gesamtsumme: {total_sum}\")\n",
    "print(f\"Summe für Wörter mit 10 oder mehr Vorkommen: {sum_at_least_10}\")\n",
    "print(f\"Summe für gesuchte Wörter: {sum_gesuchte_woerter}\")\n",
    "print(f\"Anzahl einzigartiger gefundener Wörter: {len(gefundene_woerter)}\")\n",
    "print(f\"Summe für nicht gefundene Wörter: {sum_nicht_gefundene_woerter}\")\n",
    "print(f\"Anzahl einzigartiger nicht gefundener Wörter: {len(nicht_gefundene_woerter)}\")\n",
    "#print(\"Wörter mit Übereinstimmung, sortiert nach Häufigkeit:\", sortierte_gefundene_woerter)\n",
    "#print(\"Wörter ohne Übereinstimmung, sortiert nach Häufigkeit:\", sortierte_nicht_gefundene_woerter)\n",
    "for word, count in sortierte_gefundene_woerter:\n",
    "    print(f\"{word}: {count}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
